{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Random Forests\n",
    "\n",
    "## Bagging"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "# Instantiate classification tree\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate bagging, n_jobs=-1 means that this function will run in parallel\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimator=300, n_jobs=-1)\n",
    "\n",
    "# Fit data\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict and test\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Bag Evaluation\n",
    "\n",
    "- Some instances may be sampled several times for one model\n",
    "- Some others may not be sampled at all\n",
    "- On average, 63% are sampled. The remaining 37% can be used to validate the performance without the need for CV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "# Instantiate classification tree\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate bagging, be sure to add the oob_score parameter\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimator=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "# Instantiate classification tree\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate bagging, n_jobs=-1 means that this function will run in parallel\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimator=300, n_jobs=-1)\n",
    "\n",
    "# Fit data\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict and test\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Compare to oob_score\n",
    "oob_accuracy = bc.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that train set accuracy will generally by higher than the test set accuracy. Comparing the 2 is important to ensure that there is no overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=SEED)\n",
    "\n",
    "# Instantiate random forest\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "# Fit data \n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and test\n",
    "y_pred = rf.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Feature Importance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature importance\n",
    "importances_rf = pd.Series(rf.feature_importance_, index=X.columns)\n",
    "\n",
    "# Sort importances_rf\n",
    "sorted_importnces_rf = importances_rf.sort_values()\n",
    "\n",
    "# Horizontal bar plot\n",
    "sorted_importances_rf.plot(kind=\"barh\", color=\"lightgreen\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
